{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import skimage\n",
    "from skimage import io\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from skimage import exposure, measure\n",
    "from skimage.transform import rotate\n",
    "from skimage.measure import compare_ssim as ssim\n",
    "import re\n",
    "from torch.utils.data import Dataset\n",
    "from cellvision_lib import get_model_data_splits\n",
    "import pickle\n",
    "import matplotlib.pylab as plt\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dir_to_file_lists(directory):\n",
    "    os.chdir(directory)\n",
    "    input_list = []\n",
    "    target_list = []\n",
    "    all_tifs = glob.glob(\"*.tif\")\n",
    "    input_tifs = [file for file in all_tifs if '_channel1_' in file]\n",
    "    input_tifs.sort()\n",
    "    output_tifs = [file for file in all_tifs if '_channel6_' in file]\n",
    "    output_tifs.sort()\n",
    "    return(input_tifs, output_tifs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for small dataset to overfit\n",
    "input_tifs, output_tifs = dir_to_file_lists('/gpfs/data/lionnetlab/cellvision/pilotdata/20181009-top50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, val = get_model_data_splits('/gpfs/data/lionnetlab/cellvision/pilotdata/20181009-top50', \n",
    "                                      channel = 1, \n",
    "                                      train_pp = .67, \n",
    "                                      test_pp = .165, \n",
    "                                      val_pp = .165, \n",
    "                                      seed = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_matrix_dataset(file_list, augmentation=False):\n",
    "    \"\"\"\n",
    "    funciton takes list of file names and returns list of matrices\n",
    "    list will be 6 times as long since data is flipped + rotated too\n",
    "    params:\n",
    "    augmentation: augment original training data with flips and rotations. Returns 6 times as many images.\n",
    "    \"\"\"\n",
    "    \n",
    "    mat_list = []\n",
    "    for file in file_list:\n",
    "        orig = io.imread(file)\n",
    "        mat_list.append(orig)\n",
    "        \n",
    "        if augmentation:\n",
    "            #vertical flip\n",
    "            vert_flip = orig[::-1]\n",
    "            mat_list.append(vert_flip)\n",
    "\n",
    "            #horizonal flip\n",
    "            horiz_flip = np.flip(orig,1)\n",
    "            mat_list.append(horiz_flip)\n",
    "\n",
    "            #rotate 90 degrees\n",
    "            rot_90 = rotate(orig, 90)\n",
    "            mat_list.append(rot_90)\n",
    "\n",
    "            #rotate 180 degrees\n",
    "            rot_180 = rotate(orig, 180)\n",
    "            mat_list.append(rot_180)\n",
    "\n",
    "            #rotate 270 degrees\n",
    "            rot_270 = rotate(orig, 270)\n",
    "            mat_list.append(rot_270)\n",
    "    return(mat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small dataset\n",
    "input_tifs_mats = image_to_matrix_dataset(input_tifs[:100])\n",
    "output_tifs_mats = image_to_matrix_dataset(output_tifs[:100])\n",
    "\n",
    "#full dataset\n",
    "#train_input = image_to_matrix_dataset(list(x[0] for x in train))\n",
    "#train_target = image_to_matrix_dataset(list(x[1] for x in train))\n",
    "#\n",
    "val_input = image_to_matrix_dataset(list(x[0] for x in val))\n",
    "val_target = image_to_matrix_dataset(list(x[1] for x in val))\n",
    "#\n",
    "#test_input = image_to_matrix_dataset(list(x[0] for x in test))\n",
    "#test_target = image_to_matrix_dataset(list(x[1] for x in test))\n",
    "\n",
    "single_test_input = val_input[600]\n",
    "single_test_target = val_target[600] \n",
    "\n",
    "# write to pickle\n",
    "\n",
    "# read from pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class two_image_dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, input_tifs_mats, output_tifs_mats):\n",
    "        \n",
    "        self.input_tifs_mats = input_tifs_mats\n",
    "        self.output_tifs_mats = output_tifs_mats\n",
    "        assert (len(self.input_tifs_mats) == len(self.output_tifs_mats))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_tifs_mats)\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        input_mat = self.input_tifs_mats[key]\n",
    "        output_mat = self.output_tifs_mats[key]\n",
    "        return [input_mat, output_mat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_image_collate_func(batch):\n",
    "    \"\"\"\n",
    "    function that returns input and target as tensors\n",
    "    \"\"\"\n",
    "    input_list = []\n",
    "    target_list = []\n",
    "    for datum in batch:\n",
    "        input_list.append(datum[0].astype(dtype = 'float32')/32768)\n",
    "        target_list.append(datum[1].astype(dtype = 'float32')/32768)\n",
    "    input_tensor = torch.from_numpy(np.array(input_list))\n",
    "    target_tensor = torch.from_numpy(np.array(target_list))\n",
    "    return input_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "\n",
    "# all is the mini set to develop with\n",
    "all_dataset = two_image_dataset(input_tifs_mats, output_tifs_mats)\n",
    "all_loader = torch.utils.data.DataLoader(dataset=all_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=two_image_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "# train\n",
    "#train_dataset = two_image_dataset(train_input, train_target)\n",
    "#train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "#                                           batch_size=BATCH_SIZE,\n",
    "#                                           collate_fn=two_image_collate_func,\n",
    "#                                           shuffle=False)\n",
    "## val\n",
    "#val_dataset = two_image_dataset(val_input, val_target)\n",
    "#val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "#                                           batch_size=BATCH_SIZE,\n",
    "#                                           collate_fn=two_image_collate_func,\n",
    "#                                           shuffle=False)\n",
    "## test\n",
    "#test_dataset = two_image_dataset(test_input, test_target)\n",
    "#test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "#                                           batch_size=BATCH_SIZE,\n",
    "#                                           collate_fn=two_image_collate_func,\n",
    "#                                           shuffle=False)\n",
    "\n",
    "single_test_dataset = two_image_dataset([single_test_input]*4, [single_test_target]*4)\n",
    "single_test_loader = torch.utils.data.DataLoader(dataset=single_test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=two_image_collate_func,\n",
    "                                           shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet parts here\n",
    "\n",
    "class double_conv(nn.Module):\n",
    "    '''(conv => BN => ReLU) * 2'''\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(double_conv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class inconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(inconv, self).__init__()\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(down, self).__init__()\n",
    "        self.mpconv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            double_conv(in_ch, out_ch)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mpconv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
    "        super(up, self).__init__()\n",
    "        \n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\n",
    "\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        \n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, (diffX // 2, diffX - diffX//2,\n",
    "                        diffY // 2, diffY - diffY//2))\n",
    "        \n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class outconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(outconv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNET arch here\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.inc = inconv(n_channels, 64)\n",
    "        self.down1 = down(64, 128)\n",
    "        self.down2 = down(128, 256)\n",
    "        self.down3 = down(256, 512)\n",
    "        self.down4 = down(512, 512)\n",
    "        self.up1 = up(1024, 256)\n",
    "        self.up2 = up(512, 128)\n",
    "        self.up3 = up(256, 64)\n",
    "        self.up4 = up(128, 64)\n",
    "        self.outc = outconv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), 1, 512, 512).to(device)\n",
    "        #print(x.shape)\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        x = self.outc(x)\n",
    "        return F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    returns average ssim for loader\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    ssim_list = []\n",
    "    nmse_list = []\n",
    "    model.eval()\n",
    "    for inputs, targets in loader:\n",
    "        outputs = model(inputs)\n",
    "        # to cpu\n",
    "        cur_out = outputs.cpu()\n",
    "        cur_tar = targets.cpu()\n",
    "        # get ssim for each pair\n",
    "        for i in range(outputs.shape[0]):\n",
    "            sing_out = (cur_out.data.numpy()[i,0,:,:]*32768 // 1).astype(np.int16)\n",
    "            sing_tar = (cur_tar.data.numpy()[i,:,:]*32768 // 1).astype(np.int16)\n",
    "            cur_ssim = ssim(sing_tar, sing_out, data_range=sing_out.max() - sing_out.min())\n",
    "            sing_out = np.array(sing_out, dtype='int64')\n",
    "            sing_tar = np.array(sing_tar, dtype='int64')\n",
    "            cur_nmse = (np.abs(np.square(sing_tar - sing_out))).mean(axis=None) / (np.square(sing_tar - 0)).mean(axis=None)\n",
    "            ssim_list.append(cur_ssim)\n",
    "            nmse_list.append(cur_nmse)\n",
    "    ssim_avg = sum(ssim_list) / len(ssim_list)\n",
    "    ssim_std = np.std(ssim_list)\n",
    "    nmse_avg = sum(nmse_list) / len(nmse_list)\n",
    "    nmse_std = np.std(nmse_list)\n",
    "    \n",
    "    return (ssim_avg, ssim_std, nmse_avg, nmse_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(1, 1)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0005\n",
    "num_epochs = 20\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "os.chdir('/home/cra354')\n",
    "filename = 'test.txt'\n",
    "\n",
    "for epoch in range(num_epochs): \n",
    "    for i, (inputs, targets) in enumerate(all_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets.view(-1,1,512,512).to(device))\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i > 0 and i % 20 == 0:\n",
    "            print(\"Epoch: {}, Step : {}\".format(epoch, i))\n",
    "            print(\"Training Loss : {}\".format(loss))\n",
    "            val_ssim, val_ssim_std, nmse_avg, nmse_std = test_model(all_loader, model)\n",
    "            print(\"Validation SSIM: {},Validation SSIM Standard Deviation: {} \".format(val_ssim, val_ssim_std))\n",
    "            print(\"Validation MSE: {}, Validation MSE Standard Deviation: {}\".format(nmse_avg, nmse_std))\n",
    "            # write to file so can see streaming...\n",
    "            file = open(filename, \"a\")\n",
    "            file.write(\"Epoch: {}, Step : {} \\n\".format(epoch, i))\n",
    "            file.write(\"Training Loss : {} \\n\".format(loss))\n",
    "            file.write(\"Validation SSIM: {},Validation SSIM Standard Deviation: {} \\n\".format(val_ssim, val_ssim_std))\n",
    "            file.write(\"Validation MSE: {}, Validation MSE Standard Deviation: {} \\n\".format(nmse_avg, nmse_std))\n",
    "            file.close()\n",
    "            # save model each step\n",
    "            #modelname = 'model_' + str(RUN_CHANNEL) + '_'+ str(NUM_IMAGES) +'.p'\n",
    "            #pickle.dump(model, open(modelname, \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_CHANNEL = 1\n",
    "os.chdir('/home/cra354')\n",
    "modelname = 'model_' + str(RUN_CHANNEL) + '.p'\n",
    "pickle.dump(model, open( modelname, \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_list = []\n",
    "tar_list = []\n",
    "\n",
    "for epoch in range(num_epochs): \n",
    "    for i, (inputs, targets) in enumerate(all_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets.view(-1,1,512,512).to(device))\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i > 0 and i % 20 == 0:\n",
    "            print(epoch)\n",
    "            \n",
    "            model.eval()\n",
    "            val_ssim = test_model(all_loader, model)\n",
    "            print(\"Validation SSIM: {}\".format(val_ssim))\n",
    "            cur_out = outputs.cpu()\n",
    "            cur_tar = targets.cpu()\n",
    "            # get single image from batch\n",
    "            sing_out = (cur_out.data.numpy()[0,0,:,:]*32768 // 1).astype(np.int16)\n",
    "            sing_tar = (cur_tar.data.numpy()[0,:,:]*32768 // 1).astype(np.int16)\n",
    "            # append to list\n",
    "            out_list.append(sing_out)\n",
    "            tar_list.append(sing_tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_list = out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_list = full_list[::1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, len(out_list), figsize=(16, 8))#, sharex=True, sharey=True)\n",
    "ax = axes.ravel()\n",
    "\n",
    "for i in range(len(out_list)):\n",
    "    ax[i].imshow(tar_list[i], cmap='jet')\n",
    "for i in range(len(out_list)):\n",
    "    ax[i+len(out_list)].imshow(out_list[i], cmap='jet')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(model_parameters)\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models 1-5 and test\n",
    "os.chdir('/home/cra354')\n",
    "model_1 = pickle.load( open( \"model_1.p\", \"rb\" ) )\n",
    "model_2 = pickle.load( open( \"model_2.p\", \"rb\" ) )\n",
    "model_3 = pickle.load( open( \"model_3.p\", \"rb\" ) )\n",
    "model_4 = pickle.load( open( \"model_4.p\", \"rb\" ) )\n",
    "model_5 = pickle.load( open( \"model_5.p\", \"rb\" ) )\n",
    "\n",
    "model_list = [model_1,\n",
    "            model_2,\n",
    "            model_3,\n",
    "            model_4,\n",
    "            model_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(loader=test_loader, model=model_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ssim_nmse_orig(loader):\n",
    "    ssim_list = []\n",
    "    nmse_list = []\n",
    "    for inputs, targets in loader:\n",
    "        cur_out = inputs.cpu()\n",
    "        cur_tar = targets.cpu()\n",
    "        # get ssim for each pair\n",
    "        for i in range(inputs.shape[0]):\n",
    "            #print(cur_out.data.numpy()[i])\n",
    "            #print(cur_tar.data.numpy()[i])\n",
    "            sing_out = (cur_out.data.numpy()[i]*32768 // 1).astype(np.int16)\n",
    "            sing_tar = (cur_tar.data.numpy()[i]*32768 // 1).astype(np.int16)\n",
    "            #print(sing_out == sing_tar)\n",
    "            cur_ssim = ssim(sing_tar, sing_out, data_range=(sing_out.max() - sing_out.min()))\n",
    "            #print(cur_ssim)\n",
    "            sing_out = np.array(sing_out, dtype='int64')\n",
    "            sing_tar = np.array(sing_tar, dtype='int64')\n",
    "            cur_nmse = (np.abs(np.square(sing_tar - sing_out))).mean(axis=None) / (np.square(sing_tar - 0)).mean(axis=None)\n",
    "            #print(cur_nmse)\n",
    "            ssim_list.append(cur_ssim)\n",
    "            nmse_list.append(cur_nmse)\n",
    "    ssim_avg = sum(ssim_list) / len(ssim_list)\n",
    "    ssim_std = np.std(ssim_list)\n",
    "    nmse_avg = sum(nmse_list) / len(nmse_list)\n",
    "    nmse_std = np.std(nmse_list)\n",
    "    \n",
    "    return (ssim_avg, ssim_std, nmse_avg, nmse_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, val = get_model_data_splits('/gpfs/data/lionnetlab/cellvision/pilotdata/20181009-top50', \n",
    "                                          channel = channel, \n",
    "                                          train_pp = .67, \n",
    "                                          test_pp = .165, \n",
    "                                          val_pp = .165, \n",
    "                                          seed = 1)\n",
    "val_input = image_to_matrix_dataset(list(x[0] for x in val))\n",
    "val_target = image_to_matrix_dataset(list(x[1] for x in val))\n",
    "val_dataset = two_image_dataset(val_input, val_target)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=two_image_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_ssim_nmse_orig(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "channel_list = [1,2,3,4,5]\n",
    "for channel in channel_list:\n",
    "    train, test, val = get_model_data_splits('/gpfs/data/lionnetlab/cellvision/pilotdata/20181009-top50', \n",
    "                                          channel = channel, \n",
    "                                          train_pp = .67, \n",
    "                                          test_pp = .165, \n",
    "                                          val_pp = .165, \n",
    "                                          seed = 1)\n",
    "    val_input = image_to_matrix_dataset(list(x[0] for x in val))\n",
    "    val_target = image_to_matrix_dataset(list(x[1] for x in val))\n",
    "    val_dataset = two_image_dataset(val_input, val_target)\n",
    "    val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               collate_fn=two_image_collate_func,\n",
    "                                               shuffle=False)\n",
    "    print(get_ssim_nmse_orig(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    returns average ssim for loader\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    for inputs, targets in loader:\n",
    "        outputs = model(inputs)\n",
    "        # to cpu\n",
    "        cur_out = outputs.cpu()\n",
    "        cur_tar = targets.cpu()\n",
    "        # get ssim for each pair\n",
    "        for i in range(outputs.shape[0]):\n",
    "            sing_out = (cur_out.data.numpy()[i,0,:,:]*32768 // 1).astype(np.int16)\n",
    "    return(sing_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/home/cra354/small_steps/')\n",
    "model = pickle.load( open( \"model_1_small_steps.p\", \"rb\" ) )\n",
    "out_image = single_test_model(single_test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(out_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(single_test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_test_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for count, i in enumerate(val):\n",
    "    if i[1] == '/gpfs/data/lionnetlab/cellvision/pilotdata/20181009-top50/sample31_channel6_z9.tif':\n",
    "        print (count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
