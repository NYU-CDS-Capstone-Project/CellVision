{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import skimage\n",
    "from skimage import io\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from skimage import exposure, measure\n",
    "from skimage.transform import rotate\n",
    "import re\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dir_to_file_lists(directory):\n",
    "    os.chdir(directory)\n",
    "    input_list = []\n",
    "    target_list = []\n",
    "    all_tifs = glob.glob(\"*.tif\")\n",
    "    input_tifs = [file for file in all_tifs if '_channel1_' in file]\n",
    "    input_tifs.sort()\n",
    "    output_tifs = [file for file in all_tifs if '_channel6_' in file]\n",
    "    output_tifs.sort()\n",
    "    return(input_tifs, output_tifs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tifs, output_tifs = dir_to_file_lists('/gpfs/data/lionnetlab/cellvision/pilotdata/20181009-top50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_matrix_dataset(file_list):\n",
    "    \"\"\"\n",
    "    funciton takes list of file names and returns list of matrices\n",
    "    list will be 6 times as long since data is flipped + rotated too\n",
    "    \"\"\"\n",
    "    \n",
    "    mat_list = []\n",
    "    for file in file_list:\n",
    "        orig = io.imread(file)\n",
    "        mat_list.append(orig)\n",
    "        \n",
    "        #vertical flip\n",
    "        vert_flip = orig[::-1]\n",
    "        mat_list.append(vert_flip)\n",
    "        \n",
    "        #horizonal flip\n",
    "        horiz_flip = np.flip(orig,1)\n",
    "        mat_list.append(horiz_flip)\n",
    "        \n",
    "        #rotate 90 degrees\n",
    "        rot_90 = rotate(orig, 90)\n",
    "        mat_list.append(rot_90)\n",
    "        \n",
    "        #rotate 180 degrees\n",
    "        rot_180 = rotate(orig, 180)\n",
    "        mat_list.append(rot_180)\n",
    "        \n",
    "        #rotate 270 degrees\n",
    "        rot_270 = rotate(orig, 270)\n",
    "        mat_list.append(rot_270)\n",
    "    return(mat_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tifs_mats = image_to_matrix_dataset(input_tifs[:100])\n",
    "output_tifs_mats = image_to_matrix_dataset(output_tifs[:100])\n",
    "# write to pickle\n",
    "\n",
    "# read from pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 875.,  950.,  930.,  836.,  923.,  871.,  757.,  968.,  929.,\n",
       "       1016.,  812.,  830.,  882.,  838.,  915.,  856.,  867.,  782.,\n",
       "        901.,  939.,  961.,  814.,  786.,  913.,  779.,  845., 1012.,\n",
       "        909.,  916.,  815.,  785.,  662.,  752.,  822.,  826.,  745.,\n",
       "        887.,  786.,  899.,  721., 1029.,  722.,  778.,  863.,  740.,\n",
       "        891.,  898.,  847.,  954.,  742.,  868.,  842.,  791.,  650.,\n",
       "        708.,  902.,  711.,  720.,  782.,  727.,  814.,  752.,  732.,\n",
       "        790.,  871.,  777.,  652.,  667.,  721.,  800.,  727.,  801.,\n",
       "        796.,  665.,  744.,  763.,  775.,  787.,  669.,  717.,  719.,\n",
       "        845.,  723.,  655.,  787.,  734.,  713.,  682.,  749.,  651.,\n",
       "        750.,  882.,  796.,  631.,  611.,  758.,  792.,  766.,  699.,\n",
       "        703.,  695.,  725.,  748.,  680.,  623.,  654.,  695.,  753.,\n",
       "        575.,  697.,  660.,  669.,  748.,  724.,  667.,  597.,  644.,\n",
       "        777.,  594.,  857.,  759.,  642.,  628.,  775.,  788.,  713.,\n",
       "        633.,  535.,  626.,  709.,  646.,  885.,  853.,  608.,  653.,\n",
       "        639.,  855.,  704.,  763.,  687.,  606.,  799.,  621.,  648.,\n",
       "        604.,  596.,  708.,  939.,  633.,  682.,  660.,  748.,  655.,\n",
       "        654.,  740.,  755.,  641.,  724.,  618.,  671.,  736.,  630.,\n",
       "        652.,  695.,  720.,  615.,  835.,  645.,  764.,  719.,  562.,\n",
       "        646.,  636.,  677.,  671.,  717.,  716.,  736.,  634.,  705.,\n",
       "        633.,  674.,  596.,  758.,  709.,  829.,  725.,  776.,  684.,\n",
       "        676.,  582.,  646.,  689.,  669.,  634.,  707.,  696.,  619.,\n",
       "        527.,  678.,  692.,  840.,  803.,  604.,  630.,  748.,  668.,\n",
       "        690.,  674.,  657.,  590.,  616.,  564.,  615.,  650.,  620.,\n",
       "        573.,  558.,  664.,  591.,  399.,  669.,  603.,  595.,  680.,\n",
       "        580.,  541.,  541.,  630.,  615.,  516.,  529.,  629.,  608.,\n",
       "        656.,  597.,  523.,  558.,  486.,  604.,  501.,  569.,  585.,\n",
       "        439.,  451.,  505.,  558.,  508.,  551.,  484.,  592.,  497.,\n",
       "        370.,  472.,  504.,  468.,  417.,  435.,  482.,  438.,  496.,\n",
       "        508.,  396.,  352.,  420.,  404.,  500.,  385.,  383.,  496.,\n",
       "        482.,  435.,  495.,  476.,  422.,  508.,  466.,  436.,  396.,\n",
       "        410.,  487.,  469.,  360.,  490.,  492.,  406.,  496.,  454.,\n",
       "        455.,  460.,  396.,  480.,  403.,  446.,  392.,  389.,  446.,\n",
       "        419.,  361.,  373.,  530.,  421.,  385.,  381.,  364.,  411.,\n",
       "        479.,  366.,  409.,  357.,  397.,  390.,  292.,  321.,  291.,\n",
       "        464.,  343.,  360.,  372.,  525.,  426.,  461.,  336.,  379.,\n",
       "        510.,  472.,  408.,  410.,  333.,  365.,  374.,  416.,  354.,\n",
       "        337.,  349.,  360.,  274.,  364.,  344.,  327.,  402.,  381.,\n",
       "        336.,  364.,  330.,  352.,  394.,  385.,  355.,  324.,  416.,\n",
       "        388.,  335.,  329.,  332.,  308.,  282.,  433.,  417.,  407.,\n",
       "        373.,  373.,  341.,  345.,  362.,  334.,  321.,  312.,  329.,\n",
       "        305.,  310.,  320.,  310.,  396.,  341.,  326.,  307.,  303.,\n",
       "        312.,  234.,  275.,  326.,  277.,  346.,  327.,  283.,  312.,\n",
       "        306.,  320.,  246.,  337.,  244.,  378.,  337.,  272.,  307.,\n",
       "        288.,  387.,  369.,  325.,  316.,  248.,  299.,  262.,  313.,\n",
       "        321.,  330.,  303.,  294.,  285.,  297.,  218.,  326.,  231.,\n",
       "        250.,  359.,  269.,  254.,  304.,  273.,  244.,  280.,  300.,\n",
       "        259.,  220.,  286.,  321.,  277.,  219.,  279.,  245.,  213.,\n",
       "        282.,  276.,  253.,  219.,  204.,  349.,  237.,  230.,  263.,\n",
       "        269.,  253.,  288.,  222.,  223.,  373.,  269.,  242.,  267.,\n",
       "        302.,  255.,  242.,  291.,  271.,  324.,  317.,  274.,  340.,\n",
       "        269.,  316.,  272.,  211.,  336.,  235.,  278.,  301.,  277.,\n",
       "        194.,  220.,  291.,  235.,  220.,  281.,  211.,  217.,  277.,\n",
       "        284.,  238.,  295.,  238.,  254.,  274.,  221.,  359.,  207.,\n",
       "        259.,  235.,  242.,  249.,  256.,  241.,  249.,  288.,  346.,\n",
       "        247.,  289.,  241.,  185.,  234.,  272.,  300.,  248.,  191.,\n",
       "        277.,  301.,  212.,  276.,  228.,  273.,  282.,  235.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tifs_mats[1][0].astype(dtype = 'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class two_image_dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, input_tifs_mats, output_tifs_mats):\n",
    "        \n",
    "        self.input_tifs_mats = input_tifs_mats\n",
    "        self.output_tifs_mats = output_tifs_mats\n",
    "        assert (len(self.input_tifs_mats) == len(self.output_tifs_mats))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_tifs_mats)\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        input_mat = self.input_tifs_mats[key]\n",
    "        output_mat = self.output_tifs_mats[key]\n",
    "        return [input_mat, output_mat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_image_collate_func(batch):\n",
    "    \"\"\"\n",
    "    function that returns input and target as tensors\n",
    "    \"\"\"\n",
    "    input_list = []\n",
    "    target_list = []\n",
    "    for datum in batch:\n",
    "        input_list.append(datum[0].astype(dtype = 'float32')/32768)\n",
    "        target_list.append(datum[1].astype(dtype = 'float32')/32768)\n",
    "    input_tensor = torch.from_numpy(np.array(input_list))\n",
    "    target_tensor = torch.from_numpy(np.array(input_list))\n",
    "    return [input_tensor, target_tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "all_dataset = two_image_dataset(input_tifs_mats, output_tifs_mats)\n",
    "all_loader = torch.utils.data.DataLoader(dataset=all_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=two_image_collate_func,\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#for i, (mat1, mat2) in enumerate(all_loader):\n",
    "#    print(i)\n",
    "#    print(mat1)\n",
    "#    print(mat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet parts here\n",
    "\n",
    "class double_conv(nn.Module):\n",
    "    '''(conv => BN => ReLU) * 2'''\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(double_conv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class inconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(inconv, self).__init__()\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(down, self).__init__()\n",
    "        self.mpconv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            double_conv(in_ch, out_ch)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mpconv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
    "        super(up, self).__init__()\n",
    "\n",
    "        #  would be a nice idea if the upsampling could be learned too,\n",
    "        #  but my machine do not have enough memory to handle all those weights\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\n",
    "\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        \n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, (diffX // 2, diffX - diffX//2,\n",
    "                        diffY // 2, diffY - diffY//2))\n",
    "        \n",
    "        # for padding issues, see \n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class outconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(outconv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNET arch here\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.inc = inconv(n_channels, 64)\n",
    "        self.down1 = down(64, 128)\n",
    "        self.down2 = down(128, 256)\n",
    "        self.down3 = down(256, 512)\n",
    "        self.down4 = down(512, 512)\n",
    "        self.up1 = up(1024, 256)\n",
    "        self.up2 = up(512, 128)\n",
    "        self.up3 = up(256, 64)\n",
    "        self.up4 = up(128, 64)\n",
    "        self.outc = outconv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), 1, 512, 512).to(device)\n",
    "        #print(x.shape)\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        x = self.outc(x)\n",
    "        return F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(1, 1)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "num_epochs = 5 \n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss : 0.12659518420696259\n",
      "Training Loss : 0.10929626226425171\n",
      "Training Loss : 0.072621189057827\n",
      "Training Loss : 0.05901113897562027\n",
      "Training Loss : 0.06625333428382874\n",
      "Training Loss : 0.0381472148001194\n",
      "Training Loss : 0.032305099070072174\n",
      "Training Loss : 0.04280365630984306\n",
      "Training Loss : 0.021401144564151764\n",
      "Training Loss : 0.01861284300684929\n",
      "Training Loss : 0.02628067322075367\n",
      "Training Loss : 0.013229690492153168\n",
      "Training Loss : 0.01122827734798193\n",
      "Training Loss : 0.010287940502166748\n",
      "Training Loss : 0.00688639422878623\n",
      "Training Loss : 0.006497588939964771\n",
      "Training Loss : 0.004920864012092352\n",
      "Training Loss : 0.004697423428297043\n",
      "Training Loss : 0.004643122665584087\n",
      "Training Loss : 0.003507231129333377\n",
      "Training Loss : 0.0034348531626164913\n",
      "Training Loss : 0.0034930482506752014\n",
      "Training Loss : 0.0026012668386101723\n",
      "Training Loss : 0.0026501037646085024\n",
      "Training Loss : 0.0027290810830891132\n",
      "Training Loss : 0.001981619745492935\n",
      "Training Loss : 0.002037632744759321\n",
      "Training Loss : 0.002202920150011778\n",
      "Training Loss : 0.0016804116312414408\n",
      "Training Loss : 0.0017437272472307086\n",
      "Training Loss : 0.0012194911250844598\n",
      "Training Loss : 0.0013573606265708804\n",
      "Training Loss : 0.0014685572823509574\n",
      "Training Loss : 0.0010126876877620816\n",
      "Training Loss : 0.0010954308090731502\n",
      "Training Loss : 0.0012508542276918888\n",
      "Training Loss : 0.0008741688216105103\n",
      "Training Loss : 0.0009450835059396923\n",
      "Training Loss : 0.0010988842695951462\n",
      "Training Loss : 0.000722500029951334\n",
      "Training Loss : 0.0008047620067372918\n",
      "Training Loss : 0.0009630377753637731\n",
      "Training Loss : 0.0007104034884832799\n",
      "Training Loss : 0.000808877288363874\n",
      "Training Loss : 0.0005022395052947104\n",
      "Training Loss : 0.0005934306536801159\n",
      "Training Loss : 0.0007010629051364958\n",
      "Training Loss : 0.0004407120286487043\n",
      "Training Loss : 0.0005250455578789115\n",
      "Training Loss : 0.0006268361466936767\n",
      "Training Loss : 0.0004641414270736277\n",
      "Training Loss : 0.00045404350385069847\n",
      "Training Loss : 0.0005527965840883553\n",
      "Training Loss : 0.000375481991795823\n",
      "Training Loss : 0.0004485083627514541\n",
      "Training Loss : 0.0006102323532104492\n",
      "Training Loss : 0.0003538724558893591\n",
      "Training Loss : 0.000428212049882859\n",
      "Training Loss : 0.00025254327920265496\n",
      "Training Loss : 0.000342336279572919\n",
      "Training Loss : 0.0004501874791458249\n",
      "Training Loss : 0.000254435813985765\n",
      "Training Loss : 0.00029972681659273803\n",
      "Training Loss : 0.0003737006045412272\n",
      "Training Loss : 0.00021496915724128485\n",
      "Training Loss : 0.00027978752041235566\n",
      "Training Loss : 0.00039674993604421616\n",
      "Training Loss : 0.00021132314577698708\n",
      "Training Loss : 0.0002568849886301905\n",
      "Training Loss : 0.00034285979927517474\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs): \n",
    "    for i, (inputs, targets) in enumerate(all_loader):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets.view(4,1,512,512).to(device))\n",
    "        #print(outputs[0])\n",
    "        #print(targets.view(4,1,512,512)[0])\n",
    "        #print(loss)\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #print(\"Training Loss : {}\".format(loss))\n",
    "        # validate every 10 iterations\n",
    "        if i > 0 and i % 10 == 0:\n",
    "            # validate\n",
    "            print(\"Training Loss : {}\".format(loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<filter object at 0x20008a65f160>\n",
      "13394177\n"
     ]
    }
   ],
   "source": [
    "model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "print(model_parameters)\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
